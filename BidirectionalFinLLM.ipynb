{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNj9CWRDsGw+ECx1t23AUQx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "N8g2tVFC6go7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import tiktoken\n",
        "\n",
        "#special token ids\n",
        "enc = tiktoken.get_encoding('cl100k_base')\n",
        "SOS_TOKEN_ID = enc.encode(\"<|startoftext|>\", allowed_special=\"all\")[0]\n",
        "EOS_TOKEN_ID = enc.encode(\"<|endoftext|>\", allowed_special=\"all\")[0]\n",
        "PAD_TOKEN_ID = 0\n",
        "\n",
        "# HP\n",
        "batch_size = 32\n",
        "block_size = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "vocab_size = 100_000\n",
        "n_embed = 256\n",
        "n_layers = 6\n",
        "learning_rate = 1e-3\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "n_heads = 4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/sample_data/financial_narration_data.csv\")\n",
        "\n",
        "# functions prep pairs each way,  process df into tokenised id tuples\n",
        "\n",
        "#json to str\n",
        "json_to_str_dataset = []\n",
        "for _, row in df.iterrows():\n",
        "  encoder_input_ids = enc.encode(row['prompt'])\n",
        "  decoder_target_ids = enc.encode(row['target']) + [EOS_TOKEN_ID]\n",
        "  decoder_input_ids = [SOS_TOKEN_ID] + decoder_target_ids[:-1]\n",
        "  json_to_str_dataset.append((encoder_input_ids, decoder_input_ids, decoder_target_ids))\n",
        "\n",
        "#str to json\n",
        "str_to_json_dataset = []\n",
        "for _, row in df.iterrows():\n",
        "  encoder_input_ids = enc.encode(row['target'])\n",
        "  decoder_target_ids = enc.encode(row['prompt']) + [EOS_TOKEN_ID]\n",
        "  decoder_input_ids = [SOS_TOKEN_ID] + decoder_target_ids[:-1]\n",
        "  json_to_str_dataset.append((encoder_input_ids, decoder_input_ids, decoder_target_ids))\n",
        "\n",
        "#split\n",
        "n_j2s = int(0.9 * len(json_to_str_dataset))\n",
        "train_json_to_str = json_to_str_dataset[:n_j2s]\n",
        "val_json_to_str = json_to_str_dataset[n_j2s:]\n",
        "\n",
        "n_s2j = int(0.9 * len(str_to_json_dataset))\n",
        "train_str_to_json = str_to_json_dataset[:n_s2j]\n",
        "val_str_to_json = str_to_json_dataset[n_j2s:]\n"
      ],
      "metadata": {
        "id": "CPVJVQirKKIL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# general attention head\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, n_embed, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.head_size = head_size\n",
        "\n",
        "  # takes separate q_input, k_input and v_input (correct for a generic head)\n",
        "  def forward(self, query_input, key_input, value_input, mask=None):\n",
        "    q = self.query(query_input)\n",
        "    k = self.key(key_input)\n",
        "    v = self.value(value_input)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * (self.head_size**-0.5)\n",
        "\n",
        "    if mask is not None:\n",
        "      wei = wei.masked_fill(mask == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "# multi-head attention, for selfattention with a causal flag\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_embed, n_heads, block_size, is_causal):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.head_size = n_embed // n_heads\n",
        "\n",
        "    self.heads = nn.ModuleList([\n",
        "        AttentionHead(n_embed, self.head_size) for _ in range(n_heads)\n",
        "    ])\n",
        "    self.proj = nn.Linear(n_heads * self.head_size, n_embed)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.is_causal = is_causal\n",
        "\n",
        "    if self.is_causal:\n",
        "      self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    B, T, C = x.shape\n",
        "    local_mask = mask\n",
        "\n",
        "    if self.is_causal:\n",
        "      casual_mask = self.tril[:T, :T].view(1, 1, T, T)\n",
        "      if local_mask is None:\n",
        "        local_mask = casual_mask\n",
        "      else:\n",
        "        local_mask = local_mask * casual_mask\n",
        "\n",
        "    out = torch.cat([h(x, x, x, mask=local_mask) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "# MHCA for encoder-decoder attention\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "  def __init__(self, n_heads, n_embed, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([\n",
        "        AttentionHead(n_embed, head_size) for _ in range(n_heads)\n",
        "    ])\n",
        "    self.proj = nn.Linear(n_heads * head_size, n_embed)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, query_input, key_value_input, mask=None):\n",
        "    out = torch.cat([h(query_input, key_value_input, key_value_input, mask=mask) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "8DgITDPwRQlo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4 * n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embed, n_embed),\n",
        "        nn.Dropout(0.1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.net(x)"
      ],
      "metadata": {
        "id": "4ssTntZrqhpi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, n_embed, n_heads, block_size):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_heads\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "    self.sa = MultiHeadAttention(n_embed, n_heads, block_size, is_causal=False)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    x = x + self.sa(self.ln1(x), mask=mask)\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, n_embed, n_heads, block_size):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_heads\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "    self.ln3 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    self.masked_sa = MultiHeadAttention(n_embed, n_heads, block_size, is_causal=True)\n",
        "    self.cross_sa = MultiHeadCrossAttention(n_heads, n_embed, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "  def forward(self, x, encoder_output, padding_mask=None):\n",
        "    x = x + self.masked_sa(self.ln1(x), mask=padding_mask)\n",
        "    x = x + self.cross_sa(self.ln2(x), encoder_output, mask=None)\n",
        "    x = x + self.ffwd(self.ln3(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "jNqIs7slq8r3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remember dropout for embeddings\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, n_embed, block_size, n_heads, n_layers):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(*[\n",
        "        EncoderBlock(n_embed, n_heads, block_size) for _ in range(n_layers)\n",
        "    ])\n",
        "    self.ln_f = nn.LayerNorm(n_embed)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.block_size = block_size\n",
        "\n",
        "  def forward(self, idx, padding_mask=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "    x =self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "    for block in self.blocks:\n",
        "        x = block(x, mask=padding_mask)\n",
        "\n",
        "    x = self.ln_f(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "EdksRDbMuMA2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, n_embed, block_size, n_heads, n_layers):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.ModuleList([\n",
        "        DecoderBlock(n_embed, n_heads, block_size) for _ in range(n_layers)\n",
        "    ])\n",
        "    self.ln_f = nn.LayerNorm(n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.block_size = block_size\n",
        "\n",
        "  def forward(self, idx, encoder_output, padding_mask=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "    x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x = block(x, encoder_output, padding_mask=padding_mask)\n",
        "\n",
        "      x = self.ln_f(x)\n",
        "      logits = self.lm_head(x)\n",
        "      return logits"
      ],
      "metadata": {
        "id": "vcYiGn4Ovn9w"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoderTransformer(nn.Module):\n",
        "  def __init__(self, vocab_size, n_embed, block_size, n_heads, n_layers):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(vocab_size, n_embed, block_size, n_heads, n_layers)\n",
        "    self.decoder = Decoder(vocab_size, n_embed, block_size, n_heads, n_layers)\n",
        "\n",
        "  def forward(self, encoder_input_ids, decoder_input_ids, targets=None,\n",
        "              encoder_padding_mask=None, decoder_padding_mask=None):\n",
        "      encoder_output = self.encoder(encoder_input_ids, padding_mask=encoder_padding_mask)\n",
        "      logits = self.decoder(decoder_input_ids, encoder_output, padding_mask=decoder_padding_mask)\n",
        "\n",
        "      loss = None\n",
        "      if targets is not None:\n",
        "        B, T, C = logits.shape\n",
        "        logits_reshaped = logits.view(B*T, C)\n",
        "        targets_reshaped = targets.view(B*T)\n",
        "        loss = F.cross_entropy(logits_reshaped, targets_reshaped, ignore_index = PAD_TOKEN_ID)\n",
        "\n",
        "      return logits, loss\n",
        "\n",
        "  def generate(self, encoder_input_ids, max_new_tokens, encoder_padding_mask=None):\n",
        "    encoder_output = self.encoder(encoder_input_ids, padding_mask=encoder_padding_mask)\n",
        "    # batch size\n",
        "    B = encoder_input_ids[0]\n",
        "    decoder_input_ids = torch.full((B, 1), SOS_TOKEN_ID, dtype=torch.long, device=encoder_input_ids.device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      decoder_input_cond = decoder_input_ids[:, -self.decoder.block_size:]\n",
        "      logits = self.decoder(decoder_input_cond, encoder_output)\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      decoder_input_ids = torch.cat((decoder_input_ids, next_token), dim=1)\n",
        "\n",
        "      if (next_token == EOS_TOKEN_ID).all():\n",
        "          break\n",
        "    return decoder_input_ids\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "faTCsC_6xu2r"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch_seq2seq(split, direction='json_to_str'):\n",
        "  if direction == 'json_to_str':\n",
        "      data_source = train_json_to_str if split == 'train' else val_json_to_str\n",
        "  else:\n",
        "      data_source = train_str_to_json if split == 'train' else val_str_to_json\n",
        "\n",
        "  batch_enc_in, batch_dec_in, batch_dec_tgt = [], [], []\n",
        "  indices = torch.randint(0, len(data_source), (batch_size,))\n",
        "\n",
        "  for i in indices:\n",
        "    enc_ids, dec_in_ids, dec_tgt_ids = data_source[i.item()]\n",
        "    batch_enc_in.append(torch.tensor(enc_ids, dtype=torch.long))\n",
        "    batch_dec_in.append(torch.tensor(dec_in_ids, dtype=torch.long))\n",
        "    batch_dec_tgt.append(torch.tensor(dec_tgt_ids, dtype=torch.long))\n",
        "\n",
        "  max_enc_len = min(max(len(ids) for ids in batch_enc_in), block_size)\n",
        "  max_dec_len = min(max(len(ids) for ids in batch_dec_in), block_size)\n",
        "\n",
        "  # init with pad token\n",
        "  padded_enc_inputs = torch.full((batch_size, max_enc_len), PAD_TOKEN_ID, dtype=torch.long, device=device)\n",
        "  padded_dec_inputs = torch.full((batch_size, max_dec_len), PAD_TOKEN_ID, dtype=torch.long, device=device)\n",
        "  padded_dec_targets = torch.full((batch_size, max_dec_len), PAD_TOKEN_ID, dtype=torch.long, device=device)\n",
        "\n",
        "  #masks , 1 for real token , 0 for padding\n",
        "  encoder_padding_mask = torch.zeros(batch_size, max_enc_len, max_enc_len, dtype=torch.bool, device=device)\n",
        "  decoder_padding_mask = torch.zeros(batch_size, max_dec_len, max_dec_len, dtype=torch.bool, device=device)\n",
        "\n",
        "  for i in range(batch_size):\n",
        "      current_enc_len = len(batch_enc_in[i])\n",
        "      current_dec_len = len(batch_dec_in[i])\n",
        "\n",
        "      padded_enc_inputs[i, :min(current_enc_len, max_enc_len)] = batch_enc_in[i][:min(current_enc_len, max_enc_len)]\n",
        "      padded_dec_inputs[i, :min(current_dec_len, max_dec_len)] = batch_dec_in[i][:min(current_dec_len, max_dec_len)]\n",
        "      padded_dec_targets[i, :min(current_dec_len, max_dec_len)] = batch_dec_tgt[i][:min(current_dec_len, max_dec_len)]\n",
        "\n",
        "      encoder_padding_mask[i, :min(current_enc_len, max_enc_len), :min(current_enc_len, max_enc_len)] = 1\n",
        "      decoder_padding_mask[i, :min(current_dec_len, max_dec_len), :min(current_dec_len, max_dec_len)] = 1\n",
        "\n",
        "  return padded_enc_inputs, padded_dec_inputs, padded_dec_targets, encoder_padding_mask, decoder_padding_mask"
      ],
      "metadata": {
        "id": "-fskdg3I4J9K"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#two models one for each direction\n",
        "\n",
        "model_json_to_str = EncoderDecoderTransformer(\n",
        "    vocab_size=vocab_size, n_embed=n_embed, n_heads=n_heads, n_layers=n_layers, block_size=block_size\n",
        ").to(device)\n",
        "optimiser_json_to_str = optim.AdamW(model_json_to_str.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "model_str_to_json = EncoderDecoderTransformer(\n",
        "    vocab_size=vocab_size, n_embed=n_embed, n_heads=n_heads, n_layers=n_layers, block_size=block_size\n",
        ").to(device)\n",
        "optimiser_str_to_json = optim.AdamW(model_str_to_json.parameters(), lr=learning_rate)\n",
        "\n",
        "#track best val loss for both\n",
        "best_val_loss_j2s = float('inf')\n",
        "best_val_loss_s2j = float('inf')\n",
        "\n",
        "for step in range(max_iters):\n",
        "  #training j2s\n",
        "  model_json_to_str.train()\n",
        "  enc_in_j2s, dec_in_j2s, dec_tgt_j2s, enc_mask_j2s, dec_mask_j2s = \\\n",
        "    get_batch_seq2seq('train', direction='json_to_str')\n",
        "  logits_j2s, loss_j2s = model_json_to_str(enc_in_j2s, dec_in_j2s, dec_tgt_j2s, enc_mask_j2s, dec_mask_j2s)\n",
        "  optimiser_json_to_str.zero_grad()\n",
        "  loss_j2s.backward()\n",
        "  optimiser_json_to_str.step()\n",
        "\n",
        "  #training s2j\n",
        "  model_str_to_json.train()\n",
        "  enc_in_s2j, dec_in_s2j, dec_tgt_s2j, enc_mask_s2j, dec_mask_s2j = \\\n",
        "    get_batch_seq2seq('train', direction='str_to_json')\n",
        "  logits_s2j, loss_s2j = model_str_to_json(enc_in_s2j, dec_in_s2j, dec_tgt_s2j, enc_mask_s2j, dec_mask_s2j)\n",
        "  optimiser_str_to_json.zero_grad()\n",
        "  loss_s2j.backward()\n",
        "  optimiser_str_to_json.step()\n",
        "\n",
        "  #evaluate\n",
        "  if step % eval_interval == 0:\n",
        "    model_json_to_str.eval()\n",
        "    model_str_to_json.eval()\n",
        "    with torch.no_grad():\n",
        "      #j2s validation\n",
        "      val_enc_in_j2s, val_dec_in_j2s, val_dec_tgt_j2s, val_enc_mask_j2s, val_dec_mask_j2s = \\\n",
        "        get_batch_seq2seq('val', direction='json_to_str')\n",
        "      _, val_loss_j2s = model_json_to_str(val_enc_in_j2s, val_dec_in_j2s, val_dec_tgt_j2s, val_enc_mask_j2s, val_dec_mask_j2s)\n",
        "\n",
        "      #s2j validation\n",
        "      val_enc_in_s2j, val_dec_in_s2j, val_dec_tgt_s2j, val_enc_mask_s2j, val_dec_mask_s2j = \\\n",
        "        get_batch_seq2seq('val', direction=\"str_to_json\")\n",
        "      _, val_loss_s2j = model_str_to_json(val_enc_in_s2j, val_dec_in_s2j, val_dec_tgt_s2j, val_enc_mask_s2j, val_dec_mask_s2j)\n",
        "\n",
        "      print(f\"[step {step}] J2S train loss: {loss_j2s.item():.4f} | J2S val loss: {val_loss_j2s.item():.4f}\")\n",
        "      print(f\"[step {step}] S2J train loss: {loss_s2j.item():.4f} | S2J val loss: {val_loss_s2j.item():.4f}\")\n",
        "\n",
        "      #save checkpoints\n",
        "      if val_loss_j2s.item() < best_val_loss_j2s:\n",
        "        best_val_loss_j2s = val_loss_j2s.item()\n",
        "        torch.save(model_json_to_str.state_dict(), 'best_model_json_to_str.pt')\n",
        "        print(\"~~~~ Saved best J2S model ~~~~\")\n",
        "\n",
        "      if val_loss_s2j.item() < best_val_loss_s2j:\n",
        "        best_val_loss_s2j = val_loss_s2j.item()\n",
        "        torch.save(model_str_to_json.state_dict(), 'best_model_str_to_json.pt')\n",
        "        print(\"~~~~ Saved best S2J model ~~~~\")\n",
        "\n",
        "print(\"~~~~ TRAINING COMPLETE ~~~~\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "IiRlrT0l6RuP",
        "outputId": "c65a7bfc-3e0a-4a8c-bf31-250cfc07a082"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-23-2090526158.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0menc_in_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_in_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_tgt_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_mask_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_mask_j2s\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mget_batch_seq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'json_to_str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mlogits_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_j2s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_json_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_in_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_in_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_tgt_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_mask_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_mask_j2s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0moptimiser_json_to_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mloss_j2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-19-1158621913.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_input_ids, decoder_input_ids, targets, encoder_padding_mask, decoder_padding_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlogits_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtargets_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
          ]
        }
      ]
    }
  ]
}