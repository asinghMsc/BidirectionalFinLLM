{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N8g2tVFC6go7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import tiktoken\n",
        "\n",
        "#special token ids\n",
        "enc = tiktoken.get_encoding('cl100k_base')\n",
        "SOS_TOKEN_ID = enc.encode(\"<|startoftext|>\", allowed_special=\"all\")[0]\n",
        "EOS_TOKEN_ID = enc.encode(\"<|endoftext|>\", allowed_special=\"all\")[0]\n",
        "PAD_TOKEN_ID = 0\n",
        "\n",
        "# HP\n",
        "batch_size = 32\n",
        "block_size = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "vocab_size = 100_000\n",
        "n_embed = 256\n",
        "n_layers = 4\n",
        "learning_rate = 1e-3\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "n_heads = 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CPVJVQirKKIL"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/sample_data/financial_narration_data.csv\")\n",
        "\n",
        "# functions prep pairs each way,  process df into tokenised id tuples\n",
        "\n",
        "#json to str\n",
        "json_to_str_dataset = []\n",
        "for _, row in df.iterrows():\n",
        "  encoder_input_ids = enc.encode(row['prompt'])\n",
        "  decoder_target_ids = enc.encode(row['target']) + [EOS_TOKEN_ID]\n",
        "  decoder_input_ids = [SOS_TOKEN_ID] + decoder_target_ids[:-1]\n",
        "  json_to_str_dataset.append((encoder_input_ids, decoder_input_ids, decoder_target_ids))\n",
        "\n",
        "#str to json\n",
        "str_to_json_dataset = []\n",
        "for _, row in df.iterrows():\n",
        "  encoder_input_ids = enc.encode(row['target'])\n",
        "  decoder_target_ids = enc.encode(row['prompt']) + [EOS_TOKEN_ID]\n",
        "  decoder_input_ids = [SOS_TOKEN_ID] + decoder_target_ids[:-1]\n",
        "  str_to_json_dataset.append((encoder_input_ids, decoder_input_ids, decoder_target_ids))\n",
        "\n",
        "#split\n",
        "n_j2s = int(0.9 * len(json_to_str_dataset))\n",
        "train_json_to_str = json_to_str_dataset[:n_j2s]\n",
        "val_json_to_str = json_to_str_dataset[n_j2s:]\n",
        "\n",
        "n_s2j = int(0.9 * len(str_to_json_dataset))\n",
        "train_str_to_json = str_to_json_dataset[:n_s2j]\n",
        "val_str_to_json = str_to_json_dataset[n_s2j:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8DgITDPwRQlo"
      },
      "outputs": [],
      "source": [
        "# general attention head\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, n_embed, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.head_size = head_size\n",
        "\n",
        "  # takes separate q_input, k_input and v_input (correct for a generic head)\n",
        "  def forward(self, query_input, key_input, value_input, mask=None):\n",
        "    q = self.query(query_input)\n",
        "    k = self.key(key_input)\n",
        "    v = self.value(value_input)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * (self.head_size**-0.5)\n",
        "\n",
        "    if mask is not None:\n",
        "      wei = wei.masked_fill(mask == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    out = wei @ v\n",
        "\n",
        "    return out\n",
        "\n",
        "# multi-head attention, for selfattention with a causal flag\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_embed, n_heads, block_size, is_causal):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.head_size = n_embed // n_heads\n",
        "\n",
        "    self.heads = nn.ModuleList([\n",
        "        AttentionHead(n_embed, self.head_size) for _ in range(n_heads)\n",
        "    ])\n",
        "    self.proj = nn.Linear(n_heads * self.head_size, n_embed)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.is_causal = is_causal\n",
        "\n",
        "    if self.is_causal:\n",
        "      self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    B, T, C = x.shape\n",
        "    local_mask = mask\n",
        "\n",
        "    if self.is_causal:\n",
        "      casual_mask = self.tril[:T, :T].to(x.device)\n",
        "      if local_mask is None:\n",
        "        local_mask = casual_mask\n",
        "      else:\n",
        "        if local_mask.ndim == 3:\n",
        "            local_mask = local_mask.unsqueeze(1) #reshape to (B, 1, T, T)\n",
        "        local_mask = local_mask * casual_mask\n",
        "\n",
        "    # Pass the combined mask to the attention heads\n",
        "    out = torch.cat([h(x, x, x, mask=local_mask) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "# MHCA for encoder-decoder attention\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "  def __init__(self, n_heads, n_embed, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([\n",
        "        AttentionHead(n_embed, head_size) for _ in range(n_heads)\n",
        "    ])\n",
        "    self.proj = nn.Linear(n_heads * head_size, n_embed)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, query_input, key_value_input, mask=None):\n",
        "    local_mask = mask\n",
        "    if local_mask is not None:\n",
        "        # ensure mask has the correct shape for broadcasting with attention weights (B, H, T_q, T_k)\n",
        "        if local_mask.ndim == 3:\n",
        "            local_mask = local_mask.unsqueeze(1) #reshape to (B, 1, T_q, T_k)\n",
        "\n",
        "    out = torch.cat([h(query_input, key_value_input, key_value_input, mask=local_mask) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4ssTntZrqhpi"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4 * n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embed, n_embed),\n",
        "        nn.Dropout(0.1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jNqIs7slq8r3"
      },
      "outputs": [],
      "source": [
        "# general attention head\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, n_embed, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.head_size = head_size\n",
        "\n",
        "  # takes separate q_input, k_input and v_input (correct for a generic head)\n",
        "  def forward(self, query_input, key_input, value_input, mask=None):\n",
        "\n",
        "    q = self.query(query_input) # (B, T_q, head_size)\n",
        "    k = self.key(key_input)     # (B, T_k, head_size)\n",
        "    v = self.value(value_input)   # (B, T_v, head_size)\n",
        "\n",
        "\n",
        "    # compute attention scores (weights)\n",
        "    # (B, T_q, head_size) @ (B, head_size, T_k) -> (B, T_q, T_k)\n",
        "    k_transposed = k.transpose(-2, -1)\n",
        "\n",
        "    wei = q @ k_transposed * (self.head_size**-0.5)\n",
        "\n",
        "\n",
        "\n",
        "    if mask is not None:\n",
        "      local_mask = mask\n",
        "      if local_mask.ndim == 4 and local_mask.shape[1] == 1:\n",
        "          local_mask = local_mask.squeeze(1)\n",
        "\n",
        "      wei = wei.masked_fill(local_mask == 0, float('-inf'))\n",
        "\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "\n",
        "    # attention weights to values\n",
        "    # (B, T_q, T_k) @ (B, T_v, head_size) -> (B, T_q, head_size) (since T_k == T_v)\n",
        "    # torch.bmm for explicit batch matrix multiplication\n",
        "    wei_contiguous = wei.contiguous()\n",
        "    v_contiguous = v.contiguous()\n",
        "    out = torch.bmm(wei_contiguous, v_contiguous) # (B, T_q, head_size)\n",
        "    return out\n",
        "\n",
        "# multi-head attention, for selfattention with a causal flag\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_embed, n_heads, block_size, is_causal):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.head_size = n_embed // n_heads\n",
        "\n",
        "    self.heads = nn.ModuleList([\n",
        "        AttentionHead(n_embed, self.head_size) for _ in range(n_heads)\n",
        "    ])\n",
        "    self.proj = nn.Linear(n_heads * self.head_size, n_embed)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.is_causal = is_causal\n",
        "\n",
        "    if self.is_causal:\n",
        "      self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    B, T, C = x.shape\n",
        "    local_mask = mask\n",
        "\n",
        "    if self.is_causal:\n",
        "      casual_mask = self.tril[:T, :T].to(x.device)\n",
        "      if local_mask is None:\n",
        "        local_mask = casual_mask\n",
        "      else:\n",
        "        if local_mask.ndim == 3:\n",
        "            local_mask = local_mask.unsqueeze(1)\n",
        "        elif local_mask.ndim == 4:\n",
        "\n",
        "             casual_mask = casual_mask.unsqueeze(1).expand(-1, self.n_heads, -1, -1)\n",
        "\n",
        "        local_mask = local_mask * casual_mask\n",
        "\n",
        "    out = torch.cat([h(x, x, x, mask=local_mask) for h in self.heads], dim=-1)\n",
        "\n",
        "    out = self.dropout(self.proj(out))\n",
        "\n",
        "    return out\n",
        "\n",
        "# MHCA for encoder-decoder attention\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "  def __init__(self, n_heads, n_embed, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([\n",
        "        AttentionHead(n_embed, head_size) for _ in range(n_heads)\n",
        "    ])\n",
        "    self.proj = nn.Linear(n_heads * head_size, n_embed)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, query_input, key_value_input, mask=None):\n",
        "    local_mask = mask\n",
        "    if local_mask is not None:\n",
        "        # if mask has the correct shape for broadcasting with attention weights (B, H, T_q, T_k)\n",
        "        if local_mask.ndim == 3:\n",
        "            local_mask = local_mask.unsqueeze(1) #reshape to (B, 1, T_q, T_k)\n",
        "\n",
        "    out = torch.cat([h(query_input, key_value_input, key_value_input, mask=local_mask) for h in self.heads], dim=-1)\n",
        "\n",
        "    out = self.dropout(self.proj(out))\n",
        "\n",
        "    return out\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, n_embed, n_heads, block_size):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_heads\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "    self.ln3 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    self.masked_sa = MultiHeadAttention(n_embed, n_heads, block_size, is_causal=True)\n",
        "    self.cross_sa = MultiHeadCrossAttention(n_heads, n_embed, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "  def forward(self, decoder_input, encoder_output, decoder_padding_mask=None, cross_attention_mask=None):\n",
        "    # masked self-attention (uses decoder_padding_mask)\n",
        "    # output is (B, T_dec, C)\n",
        "    ln1_out = self.ln1(decoder_input)\n",
        "    masked_sa_out = self.masked_sa(ln1_out, mask=decoder_padding_mask)\n",
        "    decoder_output = decoder_input + masked_sa_out\n",
        "\n",
        "    ln2_out = self.ln2(decoder_output)\n",
        "    cross_sa_out = self.cross_sa(ln2_out, encoder_output, mask=cross_attention_mask)\n",
        "    decoder_output = decoder_output + cross_sa_out\n",
        "\n",
        "\n",
        "    # Feed-forward network\n",
        "    ln3_out = self.ln3(decoder_output)\n",
        "    ffwd_out = self.ffwd(ln3_out)\n",
        "    decoder_output = decoder_output + ffwd_out\n",
        "\n",
        "    return decoder_output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dd885ba"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, n_embed, n_heads, block_size):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_heads\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "    self.self_attention = MultiHeadAttention(n_embed, n_heads, block_size, is_causal=False) #enc uses non-causal self-attention\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "  def forward(self, x, padding_mask=None):\n",
        "    x = x + self.self_attention(self.ln1(x), mask=padding_mask)\n",
        "\n",
        "    # Feed-forward network\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EdksRDbMuMA2"
      },
      "outputs": [],
      "source": [
        "#remember dropout for embeddings\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, n_embed, block_size, n_heads, n_layers):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(*[\n",
        "        EncoderBlock(n_embed, n_heads, block_size) for _ in range(n_layers)\n",
        "    ])\n",
        "    self.ln_f = nn.LayerNorm(n_embed)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.block_size = block_size\n",
        "\n",
        "  def forward(self, idx, padding_mask=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "    x =self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "    for i, block in enumerate(self.blocks):\n",
        "            x = block(x, padding_mask=padding_mask)\n",
        "\n",
        "\n",
        "    x = self.ln_f(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vcYiGn4Ovn9w"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, n_embed, block_size, n_heads, n_layers):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.ModuleList([\n",
        "        DecoderBlock(n_embed, n_heads, block_size) for _ in range(n_layers)\n",
        "    ])\n",
        "    self.ln_f = nn.LayerNorm(n_embed)\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.block_size = block_size\n",
        "\n",
        "  def forward(self, decoder_input_ids, encoder_output, decoder_padding_mask=None, cross_attention_mask=None):\n",
        "    B, T = decoder_input_ids.shape\n",
        "    tok_emb = self.token_embedding_table(decoder_input_ids)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=decoder_input_ids.device))\n",
        "    x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      #pass decoder_padding_mask and cross_attention_mask with correct keyword arguments\n",
        "      x = block(x, encoder_output, decoder_padding_mask=decoder_padding_mask, cross_attention_mask=cross_attention_mask)\n",
        "\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "faTCsC_6xu2r"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoderTransformer(nn.Module):\n",
        "  def __init__(self, vocab_size, n_embed, block_size, n_heads, n_layers):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(vocab_size, n_embed, block_size, n_heads, n_layers)\n",
        "    self.decoder = Decoder(vocab_size, n_embed, block_size, n_heads, n_layers)\n",
        "\n",
        "  def forward(self, encoder_input_ids, decoder_input_ids, targets=None,\n",
        "              encoder_padding_mask=None, decoder_padding_mask=None, cross_attention_mask=None):\n",
        "      encoder_output = self.encoder(encoder_input_ids, padding_mask=encoder_padding_mask)\n",
        "      logits = self.decoder(decoder_input_ids, encoder_output, decoder_padding_mask=decoder_padding_mask, cross_attention_mask=cross_attention_mask)\n",
        "\n",
        "      loss = None\n",
        "      if targets is not None:\n",
        "        B, T, C = logits.shape\n",
        "        logits_reshaped = logits.view(B*T, C)\n",
        "        targets_reshaped = targets.view(B*T)\n",
        "\n",
        "        # target values that are EOS_TOKEN_ID to PAD_TOKEN_ID so they are ignored\n",
        "        targets_reshaped[targets_reshaped == EOS_TOKEN_ID] = PAD_TOKEN_ID\n",
        "\n",
        "        loss = F.cross_entropy(logits_reshaped, targets_reshaped, ignore_index = PAD_TOKEN_ID)\n",
        "\n",
        "      return logits, loss\n",
        "\n",
        "  def generate(self, encoder_input_ids, max_new_tokens, encoder_padding_mask=None):\n",
        "    encoder_output = self.encoder(encoder_input_ids, padding_mask=encoder_padding_mask)\n",
        "    # batch size\n",
        "    B = encoder_input_ids.shape[0]\n",
        "    decoder_input_ids = torch.full((B, 1), SOS_TOKEN_ID, dtype=torch.long, device=encoder_input_ids.device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      decoder_input_cond = decoder_input_ids[:, -self.decoder.block_size:]\n",
        "\n",
        "      _, T_enc, _ = encoder_output.shape\n",
        "      _, T_dec_cond = decoder_input_cond.shape\n",
        "      cross_attention_mask = torch.ones(B, T_dec_cond, T_enc, dtype=torch.bool, device=encoder_input_ids.device)\n",
        "\n",
        "\n",
        "      logits = self.decoder(decoder_input_cond, encoder_output, cross_attention_mask=cross_attention_mask)\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      decoder_input_ids = torch.cat((decoder_input_ids, next_token), dim=1)\n",
        "\n",
        "      if (next_token == EOS_TOKEN_ID).all():\n",
        "          break\n",
        "    return decoder_input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-fskdg3I4J9K"
      },
      "outputs": [],
      "source": [
        "def get_batch_seq2seq(split, direction='json_to_str'):\n",
        "  if direction == 'json_to_str':\n",
        "      data_source = train_json_to_str if split == 'train' else val_json_to_str\n",
        "  else:\n",
        "      data_source = train_str_to_json if split == 'train' else val_str_to_json\n",
        "\n",
        "  batch_enc_in, batch_dec_in, batch_dec_tgt = [], [], []\n",
        "  indices = torch.randint(0, len(data_source), (batch_size,))\n",
        "\n",
        "  for i in indices:\n",
        "    enc_ids, dec_in_ids, dec_tgt_ids = data_source[i.item()]\n",
        "    batch_enc_in.append(torch.tensor(enc_ids, dtype=torch.long))\n",
        "    batch_dec_in.append(torch.tensor(dec_in_ids, dtype=torch.long))\n",
        "    batch_dec_tgt.append(torch.tensor(dec_tgt_ids, dtype=torch.long))\n",
        "\n",
        "  # Use pad_sequence for efficient padding\n",
        "  padded_enc_inputs = nn.utils.rnn.pad_sequence(batch_enc_in, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "  padded_dec_inputs = nn.utils.rnn.pad_sequence(batch_dec_in, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "  padded_dec_targets = nn.utils.rnn.pad_sequence(batch_dec_tgt, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "\n",
        "  #masks , 1 for real token , 0 for padding\n",
        "  # (B, T_enc) -> (B, 1, T_enc) -> (B, T_enc, T_enc)\n",
        "  encoder_padding_mask = (padded_enc_inputs != PAD_TOKEN_ID).unsqueeze(1)\n",
        "  encoder_padding_mask = encoder_padding_mask * encoder_padding_mask.transpose(-2, -1)\n",
        "\n",
        "  # (B, T_dec) -> (B, 1, T_dec) -> (B, T_dec, T_dec)\n",
        "  decoder_padding_mask = (padded_dec_inputs != PAD_TOKEN_ID).unsqueeze(1)\n",
        "  decoder_padding_mask = decoder_padding_mask * decoder_padding_mask.transpose(-2, -1)\n",
        "\n",
        "  cross_attention_mask = (padded_dec_inputs != PAD_TOKEN_ID).unsqueeze(-1) * \\\n",
        "                         (padded_enc_inputs != PAD_TOKEN_ID).unsqueeze(1)\n",
        "\n",
        "  return padded_enc_inputs.to(device), padded_dec_inputs.to(device), padded_dec_targets.to(device), \\\n",
        "         encoder_padding_mask.to(device), decoder_padding_mask.to(device), cross_attention_mask.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "IiRlrT0l6RuP",
        "outputId": "cf3a3334-0c02-4260-b106-eb0a77f05332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[step 0] J2S train loss: nan | J2S val loss: nan\n",
            "[step 0] S2J train loss: nan | S2J val loss: nan\n",
            "~~~~ Saved initial J2S model at step 0 ~~~~\n",
            "~~~~ Saved initial S2J model at step 0 ~~~~\n",
            "[step 100] J2S train loss: nan | J2S val loss: nan\n",
            "[step 100] S2J train loss: nan | S2J val loss: nan\n",
            "[step 200] J2S train loss: nan | J2S val loss: nan\n",
            "[step 200] S2J train loss: nan | S2J val loss: nan\n",
            "[step 300] J2S train loss: nan | J2S val loss: nan\n",
            "[step 300] S2J train loss: nan | S2J val loss: nan\n",
            "[step 400] J2S train loss: nan | J2S val loss: nan\n",
            "[step 400] S2J train loss: nan | S2J val loss: nan\n",
            "[step 500] J2S train loss: nan | J2S val loss: nan\n",
            "[step 500] S2J train loss: nan | S2J val loss: nan\n",
            "[step 600] J2S train loss: nan | J2S val loss: nan\n",
            "[step 600] S2J train loss: nan | S2J val loss: nan\n",
            "[step 700] J2S train loss: nan | J2S val loss: nan\n",
            "[step 700] S2J train loss: nan | S2J val loss: nan\n",
            "[step 800] J2S train loss: nan | J2S val loss: nan\n",
            "[step 800] S2J train loss: nan | S2J val loss: nan\n",
            "[step 900] J2S train loss: nan | J2S val loss: nan\n",
            "[step 900] S2J train loss: nan | S2J val loss: nan\n",
            "[step 1000] J2S train loss: nan | J2S val loss: nan\n",
            "[step 1000] S2J train loss: nan | S2J val loss: nan\n",
            "[step 1100] J2S train loss: nan | J2S val loss: nan\n",
            "[step 1100] S2J train loss: nan | S2J val loss: nan\n",
            "[step 1200] J2S train loss: nan | J2S val loss: nan\n",
            "[step 1200] S2J train loss: nan | S2J val loss: nan\n",
            "[step 1300] J2S train loss: nan | J2S val loss: nan\n",
            "[step 1300] S2J train loss: nan | S2J val loss: nan\n",
            "[step 1400] J2S train loss: nan | J2S val loss: nan\n",
            "[step 1400] S2J train loss: nan | S2J val loss: nan\n",
            "[step 1500] J2S train loss: nan | J2S val loss: nan\n",
            "[step 1500] S2J train loss: nan | S2J val loss: nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-101876860.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mlogits_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_j2s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_json_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_in_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_in_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_tgt_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_mask_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_mask_j2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_mask_j2s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0moptimiser_json_to_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mloss_j2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0moptimiser_json_to_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#two models one for each direction\n",
        "\n",
        "model_json_to_str = EncoderDecoderTransformer(\n",
        "    vocab_size=vocab_size, n_embed=n_embed, n_heads=n_heads, n_layers=n_layers, block_size=block_size\n",
        ").to(device)\n",
        "optimiser_json_to_str = optim.AdamW(model_json_to_str.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "model_str_to_json = EncoderDecoderTransformer(\n",
        "    vocab_size=vocab_size, n_embed=n_embed, n_heads=n_heads, n_layers=n_layers, block_size=block_size\n",
        ").to(device)\n",
        "optimiser_str_to_json = optim.AdamW(model_str_to_json.parameters(), lr=learning_rate)\n",
        "\n",
        "#track best val loss for both\n",
        "best_val_loss_j2s = float('inf')\n",
        "best_val_loss_s2j = float('inf')\n",
        "\n",
        "for step in range(max_iters):\n",
        "  #training j2s\n",
        "  model_json_to_str.train()\n",
        "  enc_in_j2s, dec_in_j2s, dec_tgt_j2s, enc_mask_j2s, dec_mask_j2s, cross_mask_j2s = \\\n",
        "    get_batch_seq2seq('train', direction='json_to_str')\n",
        "\n",
        "  logits_j2s, loss_j2s = model_json_to_str(enc_in_j2s, dec_in_j2s, dec_tgt_j2s, enc_mask_j2s, dec_mask_j2s, cross_mask_j2s)\n",
        "  optimiser_json_to_str.zero_grad()\n",
        "  loss_j2s.backward()\n",
        "  optimiser_json_to_str.step()\n",
        "\n",
        "  #training s2j\n",
        "  model_str_to_json.train()\n",
        "  enc_in_s2j, dec_in_s2j, dec_tgt_s2j, enc_mask_s2j, dec_mask_s2j, cross_mask_s2j = \\\n",
        "    get_batch_seq2seq('train', direction='str_to_json')\n",
        "\n",
        "  logits_s2j, loss_s2j = model_str_to_json(enc_in_s2j, dec_in_s2j, dec_tgt_s2j, enc_mask_s2j, dec_mask_s2j, cross_mask_s2j)\n",
        "  optimiser_str_to_json.zero_grad()\n",
        "  loss_s2j.backward()\n",
        "  optimiser_str_to_json.step()\n",
        "\n",
        "  #evaluate and save checkpoints\n",
        "  if step % eval_interval == 0:\n",
        "    model_json_to_str.eval()\n",
        "    model_str_to_json.eval()\n",
        "    with torch.no_grad():\n",
        "      #j2s validation\n",
        "      val_enc_in_j2s, val_dec_in_j2s, val_dec_tgt_j2s, val_enc_mask_j2s, val_dec_mask_j2s, val_cross_mask_j2s = \\\n",
        "        get_batch_seq2seq('val', direction='json_to_str')\n",
        "\n",
        "      _, val_loss_j2s = model_json_to_str(val_enc_in_j2s, val_dec_in_j2s, val_dec_tgt_j2s, val_enc_mask_j2s, val_dec_mask_j2s, val_cross_mask_j2s)\n",
        "\n",
        "      #s2j validation\n",
        "      val_enc_in_s2j, val_dec_in_s2j, val_dec_tgt_s2j, val_enc_mask_s2j, val_dec_mask_s2j, val_cross_mask_s2j = \\\n",
        "        get_batch_seq2seq('val', direction=\"str_to_json\")\n",
        "\n",
        "      _, val_loss_s2j = model_str_to_json(val_enc_in_s2j, val_dec_in_s2j, val_dec_tgt_s2j, val_enc_mask_s2j, val_dec_mask_s2j, val_cross_mask_s2j)\n",
        "\n",
        "      print(f\"[step {step}] J2S train loss: {loss_j2s.item():.4f} | J2S val loss: {val_loss_j2s.item():.4f}\")\n",
        "      print(f\"[step {step}] S2J train loss: {loss_s2j.item():.4f} | S2J val loss: {val_loss_s2j.item():.4f}\")\n",
        "\n",
        "      #save checkpoints\n",
        "      if val_loss_j2s.item() < best_val_loss_j2s:\n",
        "        best_val_loss_j2s = val_loss_j2s.item()\n",
        "        torch.save(model_json_to_str.state_dict(), 'best_model_json_to_str.pt')\n",
        "        print(\"~~~~ Saved best J2S model ~~~~\")\n",
        "\n",
        "      elif step == 0:\n",
        "          torch.save(model_json_to_str.state_dict(), 'best_model_json_to_str.pt')\n",
        "          print(\"~~~~ Saved initial J2S model at step 0 ~~~~\")\n",
        "\n",
        "\n",
        "      if val_loss_s2j.item() < best_val_loss_s2j:\n",
        "        best_val_loss_s2j = val_loss_s2j.item()\n",
        "        torch.save(model_str_to_json.state_dict(), 'best_model_str_to_json.pt')\n",
        "        print(\"~~~~ Saved best S2J model ~~~~\")\n",
        "\n",
        "      elif step == 0:\n",
        "          torch.save(model_str_to_json.state_dict(), 'best_model_str_to_json.pt')\n",
        "          print(\"~~~~ Saved initial S2J model at step 0 ~~~~\")\n",
        "\n",
        "\n",
        "print(\"~~~~ TRAINING COMPLETE ~~~~\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ab32089",
        "outputId": "03222c5f-f658-4c7d-a6b5-b303bc5eac60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing MultiHeadAttention with is_causal=False\n",
            "MultiHeadAttention (is_causal=False) output shape: torch.Size([32, 32, 32, 256])\n",
            "\n",
            "Testing MultiHeadAttention with is_causal=True\n",
            "MultiHeadAttention (is_causal=True) output shape: torch.Size([32, 32, 32, 256])\n",
            "\n",
            "Testing AttentionHead in isolation\n",
            "AttentionHead output shape: torch.Size([32, 32, 32, 64])\n"
          ]
        }
      ],
      "source": [
        "# solated test for MultiHeadAttention\n",
        "B, T, C = batch_size, block_size, n_embed\n",
        "dummy_input = torch.randn(B, T, C, device=device)\n",
        "dummy_mask = torch.ones(B, T, T, dtype=torch.bool, device=device)\n",
        "\n",
        "try:\n",
        "    print(\"Testing MultiHeadAttention with is_causal=False\")\n",
        "    mha_test_causal_false = MultiHeadAttention(n_embed, n_heads, block_size, is_causal=False).to(device)\n",
        "    output_causal_false = mha_test_causal_false(dummy_input, mask=dummy_mask)\n",
        "    print(f\"MultiHeadAttention (is_causal=False) output shape: {output_causal_false.shape}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error during MultiHeadAttention (is_causal=False) test: {e}\")\n",
        "\n",
        "try:\n",
        "    print(\"\\nTesting MultiHeadAttention with is_causal=True\")\n",
        "    mha_test_causal_true = MultiHeadAttention(n_embed, n_heads, block_size, is_causal=True).to(device)\n",
        "    output_causal_true = mha_test_causal_true(dummy_input, mask=dummy_mask)\n",
        "    print(f\"MultiHeadAttention (is_causal=True) output shape: {output_causal_true.shape}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error during MultiHeadAttention (is_causal=True) test: {e}\")\n",
        "\n",
        "try:\n",
        "    print(\"\\nTesting AttentionHead in isolation\")\n",
        "    head_test = AttentionHead(n_embed, n_embed // n_heads).to(device)\n",
        "    output_head_test = head_test(dummy_input, dummy_input, dummy_input, mask=dummy_mask)\n",
        "    print(f\"AttentionHead output shape: {output_head_test.shape}\")\n",
        "except ValueError as e:\n",
        "     print(f\"Error during AttentionHead test: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EgEfifqlPTIR"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDpb1zt8SScElB+iJTxrVY"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}